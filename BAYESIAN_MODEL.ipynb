{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from util import *\n",
    "from logistic_np import *\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense,InputLayer, LeakyReLU, BatchNormalization, Activation, Dropout, Input, Flatten, Reshape\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from EDA import myEDA\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adamax, Adam, Nadam, SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "784\n",
      "392\n",
      "(60000, 784)\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# # number of input columns\n",
    "# n_inputs = X.shape[1]\n",
    "\n",
    "# # split into train test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# X_train, y_train, x_val, y_val, X_test, y_test = get_mnist_data()\n",
    "\n",
    "\n",
    "# X_train, y_train, X_test, y_test = get_vehicle_data() #For ehicle dataset\n",
    "# X_train, val_x, y_train, val_y = train_test_split(X_train, y_train, test_size=0.33, random_state=1)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() #for hand writing\n",
    "print(X_train.shape)\n",
    "X_train, X_test = normalize_all_pixel(X_train,X_test)\n",
    "# X_train, X_test = normalize_per_pixel(X_train,X_test)\n",
    "X_train  = reshape2D(X_train)\n",
    "# val_x = reshape2D(val_x)\n",
    "X_test = reshape2D(X_test)\n",
    "# scale data\n",
    "# t = StandardScaler()\n",
    "# t.fit(X_train)\n",
    "# X_train = t.transform(X_train)\n",
    "# X_test = t.transform(X_test)\n",
    "print(X_train.shape[1])\n",
    "encoder_dim = X_train.shape[1]//2\n",
    "print(encoder_dim)\n",
    "print(X_train.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_318 (Dense)           (None, 392)               307720    \n",
      "                                                                 \n",
      " batch_normalization_130 (Ba  (None, 392)              1568      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_179 (LeakyReLU)  (None, 392)              0         \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 392)               154056    \n",
      "                                                                 \n",
      " dense_320 (Dense)           (None, 196)               77028     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 540,372\n",
      "Trainable params: 539,588\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Encoder = Sequential([\n",
    "    InputLayer(input_shape=X_train.shape[1:]),\n",
    "    Dense(encoder_dim),\n",
    "    # Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(),\n",
    "    Dense(encoder_dim),\n",
    "    Dense(encoder_dim//2)], name=\"Encoder\")\n",
    "Encoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_321 (Dense)           (None, 392)               77224     \n",
      "                                                                 \n",
      " batch_normalization_131 (Ba  (None, 392)              1568      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_180 (LeakyReLU)  (None, 392)              0         \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 392)               154056    \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 784)               308112    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 540,960\n",
      "Trainable params: 540,176\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Decoder = Sequential([\n",
    "    InputLayer(input_shape=(encoder_dim//2,)),\n",
    "    Dense(encoder_dim),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(),\n",
    "    # Dropout(0.2), \n",
    "    Dense(encoder_dim),\n",
    "    Dense(X_train.shape[1])\n",
    "    ], name=\"Decoder\")\n",
    "Decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_224 (InputLayer)      [(None, 784)]             0         \n",
      "                                                                 \n",
      " Encoder (Sequential)        (None, 196)               540372    \n",
      "                                                                 \n",
      " Decoder (Sequential)        (None, 784)               540960    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,081,332\n",
      "Trainable params: 1,079,764\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(X_train.shape[1:])\n",
    "Encoderoutput = Encoder(inp)\n",
    "Decoderoutput = Decoder(Encoderoutput)\n",
    "AutoEncodermodel = Model(inputs = inp, outputs = Decoderoutput)\n",
    "AutoEncodermodel.summary()\n",
    "AutoEncodermodel.compile(optimizer = \"adam\", loss = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1618 - val_loss: 0.1004\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0848 - val_loss: 0.0781\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0701 - val_loss: 0.0662\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0646 - val_loss: 0.0671\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.0623 - val_loss: 0.0659\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.0611 - val_loss: 0.0693\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0601 - val_loss: 0.0625\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0591 - val_loss: 0.0602\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0584 - val_loss: 0.0590\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0578 - val_loss: 0.0605\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0572 - val_loss: 0.0582\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0567 - val_loss: 0.0572\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0561 - val_loss: 0.0562\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0555 - val_loss: 0.0538\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0550 - val_loss: 0.0533\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0545 - val_loss: 0.0542\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0541 - val_loss: 0.0531\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0538 - val_loss: 0.0526\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0534 - val_loss: 0.0510\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0532 - val_loss: 0.0519\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0529 - val_loss: 0.0500\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0527 - val_loss: 0.0508\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0525 - val_loss: 0.0500\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0524 - val_loss: 0.0492\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0522 - val_loss: 0.0500\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0521 - val_loss: 0.0494\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0520 - val_loss: 0.0488\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0519 - val_loss: 0.0487\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0516 - val_loss: 0.0482\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0514 - val_loss: 0.0476\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0512 - val_loss: 0.0473\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0509 - val_loss: 0.0478\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0507 - val_loss: 0.0475\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0506 - val_loss: 0.0468\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0505 - val_loss: 0.0456\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0504 - val_loss: 0.0462\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0503 - val_loss: 0.0462\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0501 - val_loss: 0.0458\n"
     ]
    }
   ],
   "source": [
    "# Train encoder model\n",
    "callback = EarlyStopping(monitor='val_loss',patience=3)\n",
    "hist = AutoEncodermodel.fit(X_train, X_train, epochs=100, validation_data=(X_test,X_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(hist.history['loss']) + 1), hist.history['loss'], label=\"train\")\n",
    "plt.plot(range(1, len(hist.history['loss']) + 1),hist.history['val_loss'], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=inp, outputs=Encoderoutput)\n",
    "encoder.compile()\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'built'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[541], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_compress.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\vis_utils.py:429\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.utils.plot_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_model\u001b[39m(\n\u001b[0;32m    368\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m     show_layer_activations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    378\u001b[0m ):\n\u001b[0;32m    379\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a Keras model to dot format and save to a file.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m      This enables in-line display of the model plots in notebooks.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt\u001b[49m:\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         )\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_graphviz():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'built'"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(plot_model(encoder, 'encoder_compress.png', show_shapes=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n",
      "Epoch:  7\n",
      "Epoch:  8\n",
      "Epoch:  9\n",
      "Epoch:  10\n",
      "Epoch:  11\n",
      "Epoch:  12\n",
      "Epoch:  13\n",
      "Epoch:  14\n",
      "Epoch:  15\n",
      "Epoch:  16\n",
      "Epoch:  17\n",
      "Epoch:  18\n",
      "Epoch:  19\n",
      "Epoch:  20\n",
      "Epoch:  21\n",
      "Epoch:  22\n",
      "Epoch:  23\n",
      "Epoch:  24\n",
      "Epoch:  25\n",
      "Epoch:  26\n",
      "Epoch:  27\n",
      "Epoch:  28\n",
      "Epoch:  29\n",
      "Epoch:  30\n",
      "Epoch:  31\n",
      "Epoch:  32\n",
      "Epoch:  33\n",
      "Epoch:  34\n",
      "Epoch:  35\n",
      "Epoch:  36\n",
      "Epoch:  37\n",
      "Epoch:  38\n",
      "Epoch:  39\n",
      "Epoch:  40\n",
      "Epoch:  41\n",
      "Epoch:  42\n",
      "Epoch:  43\n",
      "Epoch:  44\n",
      "Epoch:  45\n",
      "Epoch:  46\n",
      "Epoch:  47\n",
      "Epoch:  48\n",
      "Epoch:  49\n",
      "Epoch:  50\n",
      "Epoch:  51\n",
      "Epoch:  52\n",
      "Epoch:  53\n",
      "Epoch:  54\n",
      "Epoch:  55\n",
      "Epoch:  56\n",
      "Epoch:  57\n",
      "Epoch:  58\n",
      "Epoch:  59\n",
      "Epoch:  60\n",
      "Epoch:  61\n",
      "Epoch:  62\n",
      "Epoch:  63\n",
      "Epoch:  64\n",
      "Epoch:  65\n",
      "Epoch:  66\n",
      "Epoch:  67\n",
      "Epoch:  68\n",
      "Epoch:  69\n",
      "Epoch:  70\n",
      "Epoch:  71\n",
      "Epoch:  72\n",
      "Epoch:  73\n",
      "Epoch:  74\n",
      "Epoch:  75\n",
      "Epoch:  76\n",
      "Epoch:  77\n",
      "Epoch:  78\n",
      "Epoch:  79\n",
      "Epoch:  80\n",
      "Epoch:  81\n",
      "Epoch:  82\n",
      "Epoch:  83\n",
      "Epoch:  84\n",
      "Epoch:  85\n",
      "Epoch:  86\n",
      "Epoch:  87\n",
      "Epoch:  88\n",
      "Epoch:  89\n",
      "Epoch:  90\n",
      "Epoch:  91\n",
      "Epoch:  92\n",
      "Epoch:  93\n",
      "Epoch:  94\n",
      "Epoch:  95\n",
      "Epoch:  96\n",
      "Epoch:  97\n",
      "Epoch:  98\n",
      "Epoch:  99\n",
      "Accuracy: 48.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gaussian = GaussianNB()\n",
    "for i in range(100):\n",
    "    gaussian.fit(X_train, y_train)\n",
    "    print(\"Epoch: \",i )\n",
    "\n",
    "y_pred = gaussian.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: %.2f%%\" %(100*accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "feature_extract = load_model('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_encoded = feature_extract.predict(X_train)\n",
    "X_test_encoded = feature_extract.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n",
      "Epoch:  7\n",
      "Epoch:  8\n",
      "Epoch:  9\n",
      "Epoch:  10\n",
      "Epoch:  11\n",
      "Epoch:  12\n",
      "Epoch:  13\n",
      "Epoch:  14\n",
      "Epoch:  15\n",
      "Epoch:  16\n",
      "Epoch:  17\n",
      "Epoch:  18\n",
      "Epoch:  19\n",
      "Epoch:  20\n",
      "Epoch:  21\n",
      "Epoch:  22\n",
      "Epoch:  23\n",
      "Epoch:  24\n",
      "Epoch:  25\n",
      "Epoch:  26\n",
      "Epoch:  27\n",
      "Epoch:  28\n",
      "Epoch:  29\n",
      "Epoch:  30\n",
      "Epoch:  31\n",
      "Epoch:  32\n",
      "Epoch:  33\n",
      "Epoch:  34\n",
      "Epoch:  35\n",
      "Epoch:  36\n",
      "Epoch:  37\n",
      "Epoch:  38\n",
      "Epoch:  39\n",
      "Epoch:  40\n",
      "Epoch:  41\n",
      "Epoch:  42\n",
      "Epoch:  43\n",
      "Epoch:  44\n",
      "Epoch:  45\n",
      "Epoch:  46\n",
      "Epoch:  47\n",
      "Epoch:  48\n",
      "Epoch:  49\n",
      "Epoch:  50\n",
      "Epoch:  51\n",
      "Epoch:  52\n",
      "Epoch:  53\n",
      "Epoch:  54\n",
      "Epoch:  55\n",
      "Epoch:  56\n",
      "Epoch:  57\n",
      "Epoch:  58\n",
      "Epoch:  59\n",
      "Epoch:  60\n",
      "Epoch:  61\n",
      "Epoch:  62\n",
      "Epoch:  63\n",
      "Epoch:  64\n",
      "Epoch:  65\n",
      "Epoch:  66\n",
      "Epoch:  67\n",
      "Epoch:  68\n",
      "Epoch:  69\n",
      "Epoch:  70\n",
      "Epoch:  71\n",
      "Epoch:  72\n",
      "Epoch:  73\n",
      "Epoch:  74\n",
      "Epoch:  75\n",
      "Epoch:  76\n",
      "Epoch:  77\n",
      "Epoch:  78\n",
      "Epoch:  79\n",
      "Epoch:  80\n",
      "Epoch:  81\n",
      "Epoch:  82\n",
      "Epoch:  83\n",
      "Epoch:  84\n",
      "Epoch:  85\n",
      "Epoch:  86\n",
      "Epoch:  87\n",
      "Epoch:  88\n",
      "Epoch:  89\n",
      "Epoch:  90\n",
      "Epoch:  91\n",
      "Epoch:  92\n",
      "Epoch:  93\n",
      "Epoch:  94\n",
      "Epoch:  95\n",
      "Epoch:  96\n",
      "Epoch:  97\n",
      "Epoch:  98\n",
      "Epoch:  99\n",
      "Epoch:  100\n",
      "Epoch:  101\n",
      "Epoch:  102\n",
      "Epoch:  103\n",
      "Epoch:  104\n",
      "Epoch:  105\n",
      "Epoch:  106\n",
      "Epoch:  107\n",
      "Epoch:  108\n",
      "Epoch:  109\n",
      "Epoch:  110\n",
      "Epoch:  111\n",
      "Epoch:  112\n",
      "Epoch:  113\n",
      "Epoch:  114\n",
      "Epoch:  115\n",
      "Epoch:  116\n",
      "Epoch:  117\n",
      "Epoch:  118\n",
      "Epoch:  119\n",
      "Epoch:  120\n",
      "Epoch:  121\n",
      "Epoch:  122\n",
      "Epoch:  123\n",
      "Epoch:  124\n",
      "Epoch:  125\n",
      "Epoch:  126\n",
      "Epoch:  127\n",
      "Epoch:  128\n",
      "Epoch:  129\n",
      "Epoch:  130\n",
      "Epoch:  131\n",
      "Epoch:  132\n",
      "Epoch:  133\n",
      "Epoch:  134\n",
      "Epoch:  135\n",
      "Epoch:  136\n",
      "Epoch:  137\n",
      "Epoch:  138\n",
      "Epoch:  139\n",
      "Epoch:  140\n",
      "Epoch:  141\n",
      "Epoch:  142\n",
      "Epoch:  143\n",
      "Epoch:  144\n",
      "Epoch:  145\n",
      "Epoch:  146\n",
      "Epoch:  147\n",
      "Epoch:  148\n",
      "Epoch:  149\n",
      "Epoch:  150\n",
      "Epoch:  151\n",
      "Epoch:  152\n",
      "Epoch:  153\n",
      "Epoch:  154\n",
      "Epoch:  155\n",
      "Epoch:  156\n",
      "Epoch:  157\n",
      "Epoch:  158\n",
      "Epoch:  159\n",
      "Epoch:  160\n",
      "Epoch:  161\n",
      "Epoch:  162\n",
      "Epoch:  163\n",
      "Epoch:  164\n",
      "Epoch:  165\n",
      "Epoch:  166\n",
      "Epoch:  167\n",
      "Epoch:  168\n",
      "Epoch:  169\n",
      "Epoch:  170\n",
      "Epoch:  171\n",
      "Epoch:  172\n",
      "Epoch:  173\n",
      "Epoch:  174\n",
      "Epoch:  175\n",
      "Epoch:  176\n",
      "Epoch:  177\n",
      "Epoch:  178\n",
      "Epoch:  179\n",
      "Epoch:  180\n",
      "Epoch:  181\n",
      "Epoch:  182\n",
      "Epoch:  183\n",
      "Epoch:  184\n",
      "Epoch:  185\n",
      "Epoch:  186\n",
      "Epoch:  187\n",
      "Epoch:  188\n",
      "Epoch:  189\n",
      "Epoch:  190\n",
      "Epoch:  191\n",
      "Epoch:  192\n",
      "Epoch:  193\n",
      "Epoch:  194\n",
      "Epoch:  195\n",
      "Epoch:  196\n",
      "Epoch:  197\n",
      "Epoch:  198\n",
      "Epoch:  199\n",
      "Epoch:  200\n",
      "Epoch:  201\n",
      "Epoch:  202\n",
      "Epoch:  203\n",
      "Epoch:  204\n",
      "Epoch:  205\n",
      "Epoch:  206\n",
      "Epoch:  207\n",
      "Epoch:  208\n",
      "Epoch:  209\n",
      "Epoch:  210\n",
      "Epoch:  211\n",
      "Epoch:  212\n",
      "Epoch:  213\n",
      "Epoch:  214\n",
      "Epoch:  215\n",
      "Epoch:  216\n",
      "Epoch:  217\n",
      "Epoch:  218\n",
      "Epoch:  219\n",
      "Epoch:  220\n",
      "Epoch:  221\n",
      "Epoch:  222\n",
      "Epoch:  223\n",
      "Epoch:  224\n",
      "Epoch:  225\n",
      "Epoch:  226\n",
      "Epoch:  227\n",
      "Epoch:  228\n",
      "Epoch:  229\n",
      "Epoch:  230\n",
      "Epoch:  231\n",
      "Epoch:  232\n",
      "Epoch:  233\n",
      "Epoch:  234\n",
      "Epoch:  235\n",
      "Epoch:  236\n",
      "Epoch:  237\n",
      "Epoch:  238\n",
      "Epoch:  239\n",
      "Epoch:  240\n",
      "Epoch:  241\n",
      "Epoch:  242\n",
      "Epoch:  243\n",
      "Epoch:  244\n",
      "Epoch:  245\n",
      "Epoch:  246\n",
      "Epoch:  247\n",
      "Epoch:  248\n",
      "Epoch:  249\n",
      "Epoch:  250\n",
      "Epoch:  251\n",
      "Epoch:  252\n",
      "Epoch:  253\n",
      "Epoch:  254\n",
      "Epoch:  255\n",
      "Epoch:  256\n",
      "Epoch:  257\n",
      "Epoch:  258\n",
      "Epoch:  259\n",
      "Epoch:  260\n",
      "Epoch:  261\n",
      "Epoch:  262\n",
      "Epoch:  263\n",
      "Epoch:  264\n",
      "Epoch:  265\n",
      "Epoch:  266\n",
      "Epoch:  267\n",
      "Epoch:  268\n",
      "Epoch:  269\n",
      "Epoch:  270\n",
      "Epoch:  271\n",
      "Epoch:  272\n",
      "Epoch:  273\n",
      "Epoch:  274\n",
      "Epoch:  275\n",
      "Epoch:  276\n",
      "Epoch:  277\n",
      "Epoch:  278\n",
      "Epoch:  279\n",
      "Epoch:  280\n",
      "Epoch:  281\n",
      "Epoch:  282\n",
      "Epoch:  283\n",
      "Epoch:  284\n",
      "Epoch:  285\n",
      "Epoch:  286\n",
      "Epoch:  287\n",
      "Epoch:  288\n",
      "Epoch:  289\n",
      "Epoch:  290\n",
      "Epoch:  291\n",
      "Epoch:  292\n",
      "Epoch:  293\n",
      "Epoch:  294\n",
      "Epoch:  295\n",
      "Epoch:  296\n",
      "Epoch:  297\n",
      "Epoch:  298\n",
      "Epoch:  299\n",
      "Accuracy: 58.13%\n"
     ]
    }
   ],
   "source": [
    "gaussian_v2 = GaussianNB()\n",
    "for i in range(300):\n",
    "    gaussian_v2.fit(X_train_encoded, y_train)\n",
    "    print(\"Epoch: \",i )\n",
    "\n",
    "y_pred_v2 = gaussian_v2.predict(X_test_encoded)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: %.2f%%\" %(100*accuracy_score(y_test, y_pred_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define model\n",
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "# fit model on training set\n",
    "model.fit(X_train, y_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(X_test)\n",
    "# calculate accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6435\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "# fit the model on the training set\n",
    "model.fit(X_train_encoded, y_train)\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test_encoded)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
